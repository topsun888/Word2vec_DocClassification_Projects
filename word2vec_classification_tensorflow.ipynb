{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本项目目的是用tensorflow里面contrib.learn工具包对文本进行词向量训练和分类，相比以前低版本的tensorflow，1.0版本提供了很多方便的高级接口。本文的数据使用了20类新闻，具体原型代码可以参看tensorflow[文本分类示例](https://github.com/tensorflow/tensorflow/blob/r1.0/tensorflow/examples/learn/text_classification.py)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 加载工具包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas\n",
    "from sklearn import metrics\n",
    "import tensorflow as tf\n",
    "\n",
    "learn = tf.contrib.learn\n",
    "from tensorflow.contrib.layers.python.layers import encoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 读取20类新闻数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import string\n",
    "import re\n",
    "news_train = fetch_20newsgroups(subset='train', shuffle=True, \n",
    "                                remove=('headers'), random_state=11)\n",
    "news_test = fetch_20newsgroups(subset='test', shuffle=True, \n",
    "                                remove=('headers'), random_state=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#清除数字和换行符，将标点符号当作词\n",
    "def cleanText(corpus):\n",
    "    for c in string.punctuation:  \n",
    "        corpus = [z.lower().replace(c,' %s '%c ) for z in corpus]\n",
    "    #corpus = [re.sub('['+string.punctuation+']', '', s) for s in corpus]\n",
    "    corpus = [re.sub('['+string.digits+']', '', z) for z in corpus]\n",
    "    corpus = [z.lower().replace('\\n','') for z in corpus]    \n",
    "    return corpus\n",
    "X_train = cleanText(news_train.data)   \n",
    "X_test = cleanText(news_test.data)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_train, y_test = news_train.target, news_test.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 读取DBpedia文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pandas.read_csv('dbpedia_csv/train.csv', header=None)\n",
    "X_train, y_train = train[2], train[0]\n",
    "test = pandas.read_csv('dbpedia_csv/test.csv', header=None)\n",
    "X_test, y_test = test[2], test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(560000,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#随机选取一部分样本进行计算\n",
    "index = np.random.choice(a=range(len(X_train)),size=160000,replace=False)\n",
    "X_train = X_train[index]\n",
    "y_train = y_train[index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 建立词库和文本向量矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MAX_DOCUMENT_LENGTH = 100\n",
    "EMBEDDING_SIZE = 30\n",
    "n_words = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words: 363501\n"
     ]
    }
   ],
   "source": [
    "# 处理词库\n",
    "vocab_processor = learn.preprocessing.VocabularyProcessor(MAX_DOCUMENT_LENGTH)\n",
    "x_train = np.array(list(vocab_processor.fit_transform(X_train)))\n",
    "x_test = np.array(list(vocab_processor.transform(X_test)))\n",
    "n_words = len(vocab_processor.vocabulary_)\n",
    "print('Total words: %d' % n_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 词袋子模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bag_of_words_model(features, target):\n",
    "  \"\"\"词袋子模型，将文章看成单词集合，忽视先后顺序\"\"\"   \n",
    "#\n",
    "  target = tf.one_hot(target, 15, 1, 0)\n",
    "  features = encoders.bow_encoder(\n",
    "      features, vocab_size=n_words, embed_dim=EMBEDDING_SIZE)\n",
    "  logits = tf.contrib.layers.fully_connected(features, 15, activation_fn=None)\n",
    "  loss = tf.contrib.losses.softmax_cross_entropy(logits, target)\n",
    "  train_op = tf.contrib.layers.optimize_loss(\n",
    "      loss,\n",
    "      tf.contrib.framework.get_global_step(),\n",
    "      optimizer='Adam',\n",
    "      learning_rate=0.01)\n",
    "  return ({\n",
    "      'class': tf.argmax(logits, 1),\n",
    "      'prob': tf.nn.softmax(logits)\n",
    "  }, loss, train_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using temporary folder as model directory: C:\\Users\\ADMINI~1\\AppData\\Local\\Temp\\2\\tmpz6y0m1a2\n",
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_task_type': None, '_environment': 'local', '_tf_random_seed': None, '_save_checkpoints_steps': None, '_evaluation_master': '', '_keep_checkpoint_every_n_hours': 10000, '_task_id': 0, '_keep_checkpoint_max': 5, '_save_summary_steps': 100, '_tf_config': gpu_options {\n",
      "  per_process_gpu_memory_fraction: 1\n",
      "}\n",
      ", '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x000001E70F210940>, '_is_chief': True, '_master': '', '_save_checkpoints_secs': 600, '_num_ps_replicas': 0}\n"
     ]
    }
   ],
   "source": [
    "model_fn = bag_of_words_model\n",
    "classifier = learn.Estimator(model_fn=model_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-11-52e5871d6bd8>:2: calling BaseEstimator.fit (from tensorflow.contrib.learn.python.learn.estimators.estimator) with x is deprecated and will be removed after 2016-12-01.\n",
      "Instructions for updating:\n",
      "Estimator is decoupled from Scikit Learn interface by moving into\n",
      "separate class SKCompat. Arguments x, y and batch_size are only\n",
      "available in the SKCompat class, Estimator will only accept input_fn.\n",
      "Example conversion:\n",
      "  est = Estimator(...) -> est = SKCompat(Estimator(...))\n",
      "WARNING:tensorflow:From <ipython-input-11-52e5871d6bd8>:2: calling BaseEstimator.fit (from tensorflow.contrib.learn.python.learn.estimators.estimator) with y is deprecated and will be removed after 2016-12-01.\n",
      "Instructions for updating:\n",
      "Estimator is decoupled from Scikit Learn interface by moving into\n",
      "separate class SKCompat. Arguments x, y and batch_size are only\n",
      "available in the SKCompat class, Estimator will only accept input_fn.\n",
      "Example conversion:\n",
      "  est = Estimator(...) -> est = SKCompat(Estimator(...))\n",
      "WARNING:tensorflow:From <ipython-input-7-f46a8871f8c0>:8: softmax_cross_entropy (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.softmax_cross_entropy instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py:247: FutureWarning: comparison to `None` will result in an elementwise object comparison in the future.\n",
      "  equality = a == b\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\losses\\python\\losses\\loss_ops.py:394: compute_weighted_loss (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.compute_weighted_loss instead.\n",
      "WARNING:tensorflow:From C:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\losses\\python\\losses\\loss_ops.py:151: add_loss (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.add_loss instead.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Saving checkpoints for 101 into C:\\Users\\ADMINI~1\\AppData\\Local\\Temp\\2\\tmpz6y0m1a2\\model.ckpt.\n",
      "INFO:tensorflow:loss = 0.0851872, step = 101\n",
      "INFO:tensorflow:Saving checkpoints for 133 into C:\\Users\\ADMINI~1\\AppData\\Local\\Temp\\2\\tmpz6y0m1a2\\model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 165 into C:\\Users\\ADMINI~1\\AppData\\Local\\Temp\\2\\tmpz6y0m1a2\\model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 196 into C:\\Users\\ADMINI~1\\AppData\\Local\\Temp\\2\\tmpz6y0m1a2\\model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 200 into C:\\Users\\ADMINI~1\\AppData\\Local\\Temp\\2\\tmpz6y0m1a2\\model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.0390103.\n",
      "WARNING:tensorflow:From <ipython-input-11-52e5871d6bd8>:5: calling BaseEstimator.predict (from tensorflow.contrib.learn.python.learn.estimators.estimator) with x is deprecated and will be removed after 2016-12-01.\n",
      "Instructions for updating:\n",
      "Estimator is decoupled from Scikit Learn interface by moving into\n",
      "separate class SKCompat. Arguments x, y and batch_size are only\n",
      "available in the SKCompat class, Estimator will only accept input_fn.\n",
      "Example conversion:\n",
      "  est = Estimator(...) -> est = SKCompat(Estimator(...))\n",
      "WARNING:tensorflow:From <ipython-input-7-f46a8871f8c0>:8: softmax_cross_entropy (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.softmax_cross_entropy instead.\n",
      "WARNING:tensorflow:From C:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\losses\\python\\losses\\loss_ops.py:394: compute_weighted_loss (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.compute_weighted_loss instead.\n",
      "WARNING:tensorflow:From C:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\losses\\python\\losses\\loss_ops.py:151: add_loss (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.add_loss instead.\n",
      "Accuracy: 0.902843\n"
     ]
    }
   ],
   "source": [
    "# Train and predict\n",
    "classifier.fit(x_train, y_train, steps=100)\n",
    "y_predicted = [\n",
    "    p['class'] for p in classifier.predict(\n",
    "        x_test, as_iterable=True)]\n",
    "score = metrics.accuracy_score(y_test, y_predicted)\n",
    "print('Accuracy: {0:f}'.format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rnn_model(features, target):\n",
    "  \"\"\"RNN模型\"\"\"\n",
    "  # 首先将单词转化成词向量\n",
    "  # 然后将每篇文章映射成词向量集合\n",
    "  word_vectors = tf.contrib.layers.embed_sequence(\n",
    "      features, vocab_size=n_words, embed_dim=EMBEDDING_SIZE, scope='words')\n",
    "\n",
    "  # Split into list of embedding per word, while removing doc length dim.\n",
    "  # word_list results to be a list of tensors [batch_size, EMBEDDING_SIZE].\n",
    "  word_list = tf.unstack(word_vectors, axis=1)\n",
    "\n",
    "  # Create a Gated Recurrent Unit cell with hidden size of EMBEDDING_SIZE.\n",
    "  cell = tf.contrib.rnn.GRUCell(EMBEDDING_SIZE)\n",
    "\n",
    "  # Create an unrolled Recurrent Neural Networks to length of\n",
    "  # MAX_DOCUMENT_LENGTH and passes word_list as inputs for each unit.\n",
    "  _, encoding = tf.contrib.rnn.static_rnn(cell, word_list, dtype=tf.float32)\n",
    "\n",
    "  # Given encoding of RNN, take encoding of last step (e.g hidden size of the\n",
    "  # neural network of last step) and pass it as features for logistic\n",
    "  # regression over output classes.\n",
    "  target = tf.one_hot(target, 15, 1, 0)\n",
    "  logits = tf.contrib.layers.fully_connected(encoding, 15, activation_fn=None)\n",
    "  loss = tf.contrib.losses.softmax_cross_entropy(logits, target)\n",
    "\n",
    "  # Create a training op.\n",
    "  train_op = tf.contrib.layers.optimize_loss(\n",
    "      loss,\n",
    "      tf.contrib.framework.get_global_step(),\n",
    "      optimizer='Adam',\n",
    "      learning_rate=0.01)\n",
    "\n",
    "  return ({\n",
    "      'class': tf.argmax(logits, 1),\n",
    "      'prob': tf.nn.softmax(logits)\n",
    "  }, loss, train_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using temporary folder as model directory: C:\\Users\\ADMINI~1\\AppData\\Local\\Temp\\2\\tmp5mw1f5gy\n",
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_keep_checkpoint_every_n_hours': 10000, '_task_type': None, '_task_id': 0, '_master': '', '_environment': 'local', '_keep_checkpoint_max': 5, '_num_ps_replicas': 0, '_save_summary_steps': 100, '_is_chief': True, '_evaluation_master': '', '_save_checkpoints_secs': 600, '_tf_config': gpu_options {\n",
      "  per_process_gpu_memory_fraction: 1\n",
      "}\n",
      ", '_save_checkpoints_steps': None, '_tf_random_seed': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x000001DB4212D208>}\n",
      "WARNING:tensorflow:From <ipython-input-25-f4499f2548fe>:4: calling BaseEstimator.fit (from tensorflow.contrib.learn.python.learn.estimators.estimator) with y is deprecated and will be removed after 2016-12-01.\n",
      "Instructions for updating:\n",
      "Estimator is decoupled from Scikit Learn interface by moving into\n",
      "separate class SKCompat. Arguments x, y and batch_size are only\n",
      "available in the SKCompat class, Estimator will only accept input_fn.\n",
      "Example conversion:\n",
      "  est = Estimator(...) -> est = SKCompat(Estimator(...))\n",
      "WARNING:tensorflow:From <ipython-input-25-f4499f2548fe>:4: calling BaseEstimator.fit (from tensorflow.contrib.learn.python.learn.estimators.estimator) with x is deprecated and will be removed after 2016-12-01.\n",
      "Instructions for updating:\n",
      "Estimator is decoupled from Scikit Learn interface by moving into\n",
      "separate class SKCompat. Arguments x, y and batch_size are only\n",
      "available in the SKCompat class, Estimator will only accept input_fn.\n",
      "Example conversion:\n",
      "  est = Estimator(...) -> est = SKCompat(Estimator(...))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py:247: FutureWarning: comparison to `None` will result in an elementwise object comparison in the future.\n",
      "  equality = a == b\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-24-b70677e1bf3b>:24: softmax_cross_entropy (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.softmax_cross_entropy instead.\n",
      "WARNING:tensorflow:From C:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\losses\\python\\losses\\loss_ops.py:394: compute_weighted_loss (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.compute_weighted_loss instead.\n",
      "WARNING:tensorflow:From C:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\losses\\python\\losses\\loss_ops.py:151: add_loss (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.add_loss instead.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into C:\\Users\\ADMINI~1\\AppData\\Local\\Temp\\2\\tmp5mw1f5gy\\model.ckpt.\n",
      "INFO:tensorflow:step = 1, loss = 2.70814\n",
      "INFO:tensorflow:Saving checkpoints for 35 into C:\\Users\\ADMINI~1\\AppData\\Local\\Temp\\2\\tmp5mw1f5gy\\model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 68 into C:\\Users\\ADMINI~1\\AppData\\Local\\Temp\\2\\tmp5mw1f5gy\\model.ckpt.\n",
      "INFO:tensorflow:Saving checkpoints for 100 into C:\\Users\\ADMINI~1\\AppData\\Local\\Temp\\2\\tmp5mw1f5gy\\model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.00187004.\n",
      "WARNING:tensorflow:From <ipython-input-25-f4499f2548fe>:7: calling BaseEstimator.predict (from tensorflow.contrib.learn.python.learn.estimators.estimator) with x is deprecated and will be removed after 2016-12-01.\n",
      "Instructions for updating:\n",
      "Estimator is decoupled from Scikit Learn interface by moving into\n",
      "separate class SKCompat. Arguments x, y and batch_size are only\n",
      "available in the SKCompat class, Estimator will only accept input_fn.\n",
      "Example conversion:\n",
      "  est = Estimator(...) -> est = SKCompat(Estimator(...))\n",
      "WARNING:tensorflow:From <ipython-input-24-b70677e1bf3b>:24: softmax_cross_entropy (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.softmax_cross_entropy instead.\n",
      "WARNING:tensorflow:From C:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\losses\\python\\losses\\loss_ops.py:394: compute_weighted_loss (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.compute_weighted_loss instead.\n",
      "WARNING:tensorflow:From C:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\losses\\python\\losses\\loss_ops.py:151: add_loss (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.add_loss instead.\n",
      "Accuracy: 0.821886\n"
     ]
    }
   ],
   "source": [
    "model_fn = rnn_model\n",
    "classifier = learn.Estimator(model_fn=model_fn)\n",
    "# Train and predict\n",
    "classifier.fit(x_train, y_train, steps=100)\n",
    "y_predicted = [\n",
    "    p['class'] for p in classifier.predict(\n",
    "        x_test, as_iterable=True)]\n",
    "score = metrics.accuracy_score(y_test, y_predicted)\n",
    "print('Accuracy: {0:f}'.format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N_FILTERS = 10\n",
    "WINDOW_SIZE = 20\n",
    "FILTER_SHAPE1 = [WINDOW_SIZE, EMBEDDING_SIZE]\n",
    "FILTER_SHAPE2 = [WINDOW_SIZE, N_FILTERS]\n",
    "POOLING_WINDOW = 4\n",
    "POOLING_STRIDE = 2\n",
    "def cnn_model(features, target):\n",
    "  \"\"\"2 layer ConvNet to predict from sequence of words to a class.\"\"\"\n",
    "  # Convert indexes of words into embeddings.\n",
    "  # This creates embeddings matrix of [n_words, EMBEDDING_SIZE] and then\n",
    "  # maps word indexes of the sequence into [batch_size, sequence_length,\n",
    "  # EMBEDDING_SIZE].\n",
    "  target = tf.one_hot(target, 15, 1, 0)\n",
    "  word_vectors = tf.contrib.layers.embed_sequence(\n",
    "      features, vocab_size=n_words, embed_dim=EMBEDDING_SIZE, scope='words')\n",
    "  word_vectors = tf.expand_dims(word_vectors, 3)\n",
    "  with tf.variable_scope('CNN_Layer1'):\n",
    "    # Apply Convolution filtering on input sequence.\n",
    "    conv1 = tf.contrib.layers.convolution2d(\n",
    "        word_vectors, N_FILTERS, FILTER_SHAPE1, padding='VALID')\n",
    "    # Add a RELU for non linearity.\n",
    "    conv1 = tf.nn.relu(conv1)\n",
    "    # Max pooling across output of Convolution+Relu.\n",
    "    pool1 = tf.nn.max_pool(\n",
    "        conv1,\n",
    "        ksize=[1, POOLING_WINDOW, 1, 1],\n",
    "        strides=[1, POOLING_STRIDE, 1, 1],\n",
    "        padding='SAME')\n",
    "    # Transpose matrix so that n_filters from convolution becomes width.\n",
    "    pool1 = tf.transpose(pool1, [0, 1, 3, 2])\n",
    "  with tf.variable_scope('CNN_Layer2'):\n",
    "    # Second level of convolution filtering.\n",
    "    conv2 = tf.contrib.layers.convolution2d(\n",
    "        pool1, N_FILTERS, FILTER_SHAPE2, padding='VALID')\n",
    "    # Max across each filter to get useful features for classification.\n",
    "    pool2 = tf.squeeze(tf.reduce_max(conv2, 1), squeeze_dims=[1])\n",
    "\n",
    "  # Apply regular WX + B and classification.\n",
    "  logits = tf.contrib.layers.fully_connected(pool2, 15, activation_fn=None)\n",
    "  loss = tf.contrib.losses.softmax_cross_entropy(logits, target)\n",
    "\n",
    "  train_op = tf.contrib.layers.optimize_loss(\n",
    "      loss,\n",
    "      tf.contrib.framework.get_global_step(),\n",
    "      optimizer='Adam',\n",
    "      learning_rate=0.01)\n",
    "\n",
    "  return ({\n",
    "      'class': tf.argmax(logits, 1),\n",
    "      'prob': tf.nn.softmax(logits)\n",
    "  }, loss, train_op)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using temporary folder as model directory: C:\\Users\\ADMINI~1\\AppData\\Local\\Temp\\2\\tmp7jzlpwjd\n",
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_keep_checkpoint_every_n_hours': 10000, '_task_type': None, '_task_id': 0, '_master': '', '_environment': 'local', '_keep_checkpoint_max': 5, '_num_ps_replicas': 0, '_save_summary_steps': 100, '_is_chief': True, '_evaluation_master': '', '_save_checkpoints_secs': 600, '_tf_config': gpu_options {\n",
      "  per_process_gpu_memory_fraction: 1\n",
      "}\n",
      ", '_save_checkpoints_steps': None, '_tf_random_seed': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x000001DB03745780>}\n",
      "WARNING:tensorflow:From <ipython-input-33-a38750330311>:4: calling BaseEstimator.fit (from tensorflow.contrib.learn.python.learn.estimators.estimator) with y is deprecated and will be removed after 2016-12-01.\n",
      "Instructions for updating:\n",
      "Estimator is decoupled from Scikit Learn interface by moving into\n",
      "separate class SKCompat. Arguments x, y and batch_size are only\n",
      "available in the SKCompat class, Estimator will only accept input_fn.\n",
      "Example conversion:\n",
      "  est = Estimator(...) -> est = SKCompat(Estimator(...))\n",
      "WARNING:tensorflow:From <ipython-input-33-a38750330311>:4: calling BaseEstimator.fit (from tensorflow.contrib.learn.python.learn.estimators.estimator) with x is deprecated and will be removed after 2016-12-01.\n",
      "Instructions for updating:\n",
      "Estimator is decoupled from Scikit Learn interface by moving into\n",
      "separate class SKCompat. Arguments x, y and batch_size are only\n",
      "available in the SKCompat class, Estimator will only accept input_fn.\n",
      "Example conversion:\n",
      "  est = Estimator(...) -> est = SKCompat(Estimator(...))\n",
      "WARNING:tensorflow:From <ipython-input-32-01e6b826adb4>:40: softmax_cross_entropy (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.softmax_cross_entropy instead.\n",
      "WARNING:tensorflow:From C:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\losses\\python\\losses\\loss_ops.py:394: compute_weighted_loss (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.compute_weighted_loss instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py:247: FutureWarning: comparison to `None` will result in an elementwise object comparison in the future.\n",
      "  equality = a == b\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\losses\\python\\losses\\loss_ops.py:151: add_loss (from tensorflow.contrib.losses.python.losses.loss_ops) is deprecated and will be removed after 2016-12-30.\n",
      "Instructions for updating:\n",
      "Use tf.losses.add_loss instead.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n"
     ]
    }
   ],
   "source": [
    "model_fn = cnn_model\n",
    "classifier = learn.Estimator(model_fn=model_fn)\n",
    "# Train and predict\n",
    "classifier.fit(x_train, y_train, steps=100)\n",
    "y_predicted = [\n",
    "    p['class'] for p in classifier.predict(\n",
    "        x_test, as_iterable=True)]\n",
    "score = metrics.accuracy_score(y_test, y_predicted)\n",
    "print('Accuracy: {0:f}'.format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
